{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r './requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the data files, sign up for an account on Kaggle if you haven't already, then download the dataset here\n",
    "# https://www.kaggle.com/datasets/ashishsinhaiitr/lord-of-the-rings-text?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_ENVIRONMENT = os.environ['PINECONE_ENVIRONMENT']\n",
    "PINECONE_INDEX_NAME = os.environ['PINECONE_INDEX_NAME']\n",
    "EMBEDDING_MODEL =  os.environ['EMBEDDING_MODEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up OpenAI Embedding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "import textwrap\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "\n",
    "# let's make sure to not retry on an invalid request, because that is what we want to demonstrate\n",
    "# @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\n",
    "def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n",
    "    return openai.Embedding.create(input=text_or_tokens, model=model)\n",
    "\n",
    "def chunk_text(text: str, max_chunk_size: int, overlap_size: int) -> List[str]:\n",
    "    \"\"\"Helper function to chunk a text into overlapping chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_chunk_size - overlap_size\n",
    "    return chunks\n",
    "\n",
    "def transform_record(record: dict) -> List[dict]:\n",
    "    \"\"\"Transform a single record as described in the prompt.\"\"\"\n",
    "    max_chunk_size = 500\n",
    "    overlap_size = 100\n",
    "    chunks = chunk_text(record, max_chunk_size, overlap_size)\n",
    "    transformed_records = []\n",
    "    recordId = str(uuid4())\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{recordId}-{i+1}\"\n",
    "        transformed_records.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_parent_id': recordId,\n",
    "            'chunk_text': chunk,\n",
    "            'vector' : get_embedding(chunk).get('data')[0]['embedding']\n",
    "            #'sparse_values': splade(chunk)\n",
    "        })\n",
    "    return transformed_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT  # may be different, check at app.pinecone.io\n",
    ")\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['unused']},\n",
    "        pod_type='p2.x1'\n",
    "    )\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and load data (LOTR Fellowship of the Ring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./LOTR/01 - The Fellowship Of The Ring.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings and Pickle the results to save money on OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "chunk_array = transform_record(file)\n",
    "for chunk in chunk_array:\n",
    "    chunked_data.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data and vectors offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Pickle the array\n",
    "with open('LOTR_vector_data.pickle_500_100_sparse_no_splade', 'wb') as f:\n",
    "    pickle.dump(chunked_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from local to upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('LOTR_vector_data.pickle_500_100_sparse_no_splade', 'rb') as f:\n",
    "    vector_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format data to load to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_entries_for_pinecone(entries):\n",
    "    \"\"\"\n",
    "    Prepares an array of entries for upsert to Pinecone.\n",
    "    Each entry should have a 'vector' field containing a list of floats.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for entry in entries:\n",
    "        vector = entry['vector']\n",
    "        id = entry.get('chunk_id', '')\n",
    "        metadata = entry.get('metadata', {'chunk_id': entry.get('chunk_id', ''),'parent_id': entry.get('chunk_parent_id', ''), 'chunk_text': entry.get('chunk_text', '')})\n",
    "        values = [v for v in vector]\n",
    "        # sparse_values = entry['sparse_values']\n",
    "        #vectors.append({'id': id, 'metadata': metadata, 'values': values, 'sparse_values': sparse_values})\n",
    "        vectors.append({'id': id, 'metadata': metadata, 'values': values})\n",
    "    return {'vectors': vectors, 'namespace': ''}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = prepare_entries_for_pinecone(vector_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsert vectors (sparse and dense) and metadata to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "batch_size = 32  # process everything in batches of 32\n",
    "for i in tqdm(range(0, len(vectors['vectors']), batch_size)):\n",
    "    ids_batch = [id['id'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    embeds = [id['values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    meta = [id['metadata'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    # sparse_values = [id['sparse_values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    upserts = []\n",
    "    # loop through the data and create dictionaries for uploading documents to pinecone index\n",
    "    # for _id, sparse, dense, meta in zip(ids_batch, sparse_values, embeds, meta):\n",
    "    for _id,dense, meta in zip(ids_batch, embeds, meta):\n",
    "        upserts.append({\n",
    "            'id': _id,\n",
    "            # 'sparse_values': sparse,\n",
    "            'values': dense,\n",
    "            'metadata': meta\n",
    "        })\n",
    "    # upload the documents to the new hybrid index\n",
    "    index.upsert(upserts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 8000\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "    #sq = splade(query)\n",
    "\n",
    "\n",
    "    # get relevant contexts\n",
    "    #res = index.query(xq, top_k=5, include_metadata=True, sparse_vector=sq)\n",
    "    res = index.query(xq, top_k=5, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['chunk_text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below. If you cannot answer based on the context or general knowledge about J.R.R. Tolkien's Lord of the Rings The Fellowship of the Ring, truthfully answer that you don't know.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Memory for conversation chat style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=chat_model, \n",
    "    memory=ConversationSummaryBufferMemory(llm=chat_model, max_token_limit=650)\n",
    ")\n",
    "#conversation_with_summary.predict(input=\"Hi, what's up?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample query to Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"Who is Bilbo?\"\n",
    "# first we retrieve relevant items from Pinecone\n",
    "query_with_contexts = retrieve(query)\n",
    "print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear conversation memory if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation_with_summary.memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop to ask multiple questions and get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Prompt user for input\n",
    "    user_input = input(\"Enter your input (type 'quit' to exit): \")\n",
    "\n",
    "    # Check if user wants to quit\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Exiting program...\")\n",
    "        break\n",
    "\n",
    "    # Process user input\n",
    "    processed_input = user_input.upper()  # Convert to all uppercase letters\n",
    "    print(\"Processed input: \", processed_input)\n",
    "\n",
    "    query = user_input\n",
    "\n",
    "    # first we retrieve relevant items from Pinecone\n",
    "    query_with_contexts = retrieve(query)\n",
    "\n",
    "    # then we send the context and the query to OpenAI\n",
    "    print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
